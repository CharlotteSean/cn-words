{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crKEqghalec_"
   },
   "source": [
    "### 下载维基中文语料并训练简单的词嵌入<br />\n",
    "主要步骤\n",
    "- 下载维基中文语料\n",
    "- 抽取中文文本\n",
    "- 转换繁体为简体\n",
    "- 训练词嵌入<br />\n",
    "\n",
    "需要安装的库\n",
    "- jieba分词\n",
    "- wikiextractor 抽取中文文本\n",
    "- opencc-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oM1-OMealedA"
   },
   "outputs": [],
   "source": [
    "# 下载维基数据\n",
    "! wget -c https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YlcJwcp0ledD"
   },
   "outputs": [],
   "source": [
    "# 安装抽取和转换工具\n",
    "! pip install jieba\n",
    "! git clone https://github.com/attardi/wikiextractor.git\n",
    "! cd wikiextractor && python setup.py install\n",
    "# MacOS 可直接用Homebrew装，其它系统可查看原项目https://github.com/BYVoid/OpenCC\n",
    "! brew install OpenCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "efI39kL8ledF"
   },
   "outputs": [],
   "source": [
    "# 在wikiextractor目录下抽取维基文字，http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n",
    "! cd wikiextractor && python WikiExtractor.py -b 500M -o ../ ../zhwiki-latest-pages-articles1.xml-p1p162886.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lzEAv5bRledH"
   },
   "outputs": [],
   "source": [
    "# 使用opencc 将繁体转换为简体\n",
    "! opencc -i input_filename -o output_filename -c t2s.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18AknYgaledL"
   },
   "source": [
    "## 部分文本演示\n",
    "也可以直接下载已经转换好的部分文本（180MB/1.6GB）：https://drive.google.com/open?id=1ORNDviCeIIiosE_XlEEJfJHp0F01mtsQ\n",
    "\n",
    "下面以这部分文本做演示：[Colab地址](https://colab.research.google.com/drive/11qGq-rqv-tnvATUmcGhH0rqMSBqCWVxz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hzIqAb8mMfpL"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive \n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bFBiKhQ8Mluv"
   },
   "outputs": [],
   "source": [
    "extraed = '1ORNDviCeIIiosE_XlEEJfJHp0F01mtsQ'\n",
    "txt = drive.CreateFile({'id':extraed})\n",
    "txt.GetContentFile('wiki_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "Qx6s38uvlmwb",
    "outputId": "accdf982-13e8-4c0a-9a6b-ef3f99b624f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.3MB 5.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Running setup.py bdist_wheel for jieba ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c9/c7/63/a9ec0322ccc7c365fd51e475942a82395807186e94f0522243\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.39\n"
     ]
    }
   ],
   "source": [
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "DULbihonym73",
    "outputId": "834aae97-ebce-4f41-e398-f1ca5206b205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-05 05:07:14--  http://horatio-jsy.oss-cn-beijing.aliyuncs.com/seg_dict.txt\n",
      "Resolving horatio-jsy.oss-cn-beijing.aliyuncs.com (horatio-jsy.oss-cn-beijing.aliyuncs.com)... 59.110.190.32, 59.110.190.36\n",
      "Connecting to horatio-jsy.oss-cn-beijing.aliyuncs.com (horatio-jsy.oss-cn-beijing.aliyuncs.com)|59.110.190.32|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11167 (11K) [text/plain]\n",
      "Saving to: ‘seg_dict.txt’\n",
      "\n",
      "\r",
      "seg_dict.txt          0%[                    ]       0  --.-KB/s               \r",
      "seg_dict.txt        100%[===================>]  10.91K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-05 05:07:14 (213 MB/s) - ‘seg_dict.txt’ saved [11167/11167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -c http://horatio-jsy.oss-cn-beijing.aliyuncs.com/seg_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "mdy5QNMcledM",
    "outputId": "26feeb47-e02e-41dc-9227-8360157b4d7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.104 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgement time: 361.9946744441986\n",
      "total number of words: 37541365\n",
      "total number of words after preprocess: 17703257\n",
      "data size 17703257\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('./wiki_text.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read()\n",
    "jieba.load_userdict(\"./seg_dict.txt\")\n",
    "\n",
    "def read_data(txt):\n",
    "    seg_list = []\n",
    "    start = time.time()\n",
    "    cut_list = jieba.lcut(txt)\n",
    "    print('sgement time:', time.time()-start)\n",
    "    print('total number of words:', len(cut_list))\n",
    "\n",
    "    except_no = re.compile(r'[\\u4e00-\\u9fa5]{2,}')\n",
    "    for i in cut_list:\n",
    "        if except_no.search(i) is not None:\n",
    "            seg_list.append(i)\n",
    "    print('total number of words after preprocess:', len(seg_list))\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "\n",
    "# 返回的words是一个列表，每一个元素是一个单词字符\n",
    "words = read_data(txt)\n",
    "print('data size', len(words))\n",
    "\n",
    "# 节省分词的时间\n",
    "import pickle\n",
    "with open('words_data_saving.p', 'wb') as f:\n",
    "    pickle.dump(words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hF2NKTn_s48U"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./words_data_saving.p', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5426
    },
    "colab_type": "code",
    "id": "t8ApFPwJledR",
    "outputId": "738a1043-f598-4dba-9ad6-76ab3f49aac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words(+UNK) [['UNK', 734256], ('一个', 65764), ('中国', 61702), ('可以', 47089), ('成为', 40047)]\n",
      "Sample data [426, 426, 426, 242, 145465, 30, 430, 224, 340, 10] ['数学', '数学', '数学', '利用', '符号语言', '研究', '数量', '结构', '变化', '以及']\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "initialized\n",
      "training time: 2.323299\n",
      "Nearest to 发射: 欧尼尔 赵人 关中地区 六项 水准 收分 细化 矛尾鱼\n",
      "Nearest to 汉中: 蒋村 于夫罗 柴咲幸 留长 巨宅 王世杰 由班 洛矶\n",
      "Nearest to 而来: 各个方面 劫材 要少 巨港 驼峰 菁英 赛金花 独子\n",
      "Nearest to 飞机: 吴玉章 施南生 最早 僧伽罗语 王莽篡汉 再论 西套 同床异梦\n",
      "Nearest to 大规模: 云雾茶 六普 王岐山 调和 断袖 脚踏实地 冀州市 寿永\n",
      "Nearest to 诺贝尔奖: 蒙帕纳斯 大洋河 每过 李总统 词中 帝尧 衔接 刺毛\n",
      "Nearest to 宁波: 穆德 女护士 中国矿业 正典 过路 左至 帝女花 缺钱\n",
      "Nearest to 做法: 想像 李等 代领 战死 葵花 议局 剪应力 身高\n",
      "Nearest to 福建: 愈演愈烈 闭眼 甲子日 乔达诺 西平 帝皇 苏美英 细胞膜\n",
      "Nearest to 收看: 常见病 横切面 实习生 默写 行装 回龙观 曲尺形 诉讼案\n",
      "Nearest to 付费: 探病 诸地 哈利波 晋铎 起才 王司马亮 女权主义 拉丁舞\n",
      "Nearest to 科举: 西北工业大学 荚果 石库门 出于 回放 负离子 检票 责骂\n",
      "Nearest to 撰写: 秦公 观测员 巴罗 重内 巴伯 加班费 暂存器 王屋山\n",
      "Nearest to 距离: 三弟 齐氏 两乡 道森 伦敦桥 西斯科 新历 思想进步\n",
      "Nearest to 正确: 江岸区 忠次 费加罗 时均 桑吉 如高 职业高中 卡盖拉\n",
      "Nearest to 商务: 同位 边防军 王荷波 六八 相比 泾川县 积冰 清溪镇\n",
      "Nearest to 透明: 克拉伦斯 北进 宫古岛 糖价 左慈 滨洲 庭中 正下方\n",
      "Nearest to 乘积: 栩栩如生 中央美术学院 东汉时期 红七军 回本 加略 安福系 北及\n",
      "Nearest to 山脉: 无以为继 西尔斯 成棒 故曰 礼盒 补贴费 昂多赫 元熙\n",
      "Nearest to 说法: 黄百鸣 温为 金木水火土 第一发 王作荣 织布厂 粮库 关键\n",
      "Nearest to 以色列: 捐款 入浙 玲音 布赖特 美院 无翼鸟 双数 胡政之\n",
      "Nearest to 亿港元: 辖县 阿尼哥 西河 镜湖 胁迫 杂糅 认识论 快歌\n",
      "Nearest to 共济会: 梦家 腕足动物 遍布全国 聚四氟乙烯 由人 前应 克里米亚半岛 围廊\n",
      "Nearest to 原创: 喜子 联军部队 接力赛 松锦之战 阅报 狗不理 华东理工大学 伦西亚\n",
      "Nearest to 总督: 柯尔 阿拉善 五十两 鸡血 小室 上海 美籍华人 宜君县\n",
      "Nearest to 圣徒: 近处 手令 菜蔬 丁文江 居上 葛莱美 大周 吴士\n",
      "Nearest to 法兰西: 尸检 零次 鲨堡 陈盈豪 高中毕业 众议员 艺术院校 兴风作浪\n",
      "Nearest to 变法: 梁惠王 变时 噶玛兰族 制刀 阿根廷政府 很易 乙烯 相互辉映\n",
      "Nearest to 模型: 允诺 七重 吮吸 小水滴 王通 菲利浦 林育南 受让人\n",
      "Nearest to 年前: 胺类 七度 范阳郡 陶朗加 因祸得福 持仓 变天 水壶\n",
      "Nearest to 字节: 马丁尼 三鲜 荷叶 军政大学 通识 宋属 遗风 示范作用\n",
      "Nearest to 福音: 隶属于 右昌 葛罗 日量 李泰祥 铁管 操纵台 影视作品\n",
      "Average loss at 4000 is 26.435554\n",
      "Average loss at 8000 is 21.899648\n",
      "Average loss at 12000 is 20.114603\n",
      "Average loss at 16000 is 18.533495\n",
      "Average loss at 20000 is 17.536881\n",
      "Average loss at 24000 is 17.226742\n",
      "Average loss at 28000 is 16.305904\n",
      "Average loss at 32000 is 16.036733\n",
      "Average loss at 36000 is 15.311479\n",
      "Average loss at 40000 is 14.945440\n",
      "Average loss at 44000 is 14.768349\n",
      "Average loss at 48000 is 14.475812\n",
      "training time: 919.712890\n",
      "Nearest to 发射: 一系列 自身 距离 观察 相对 比较 但是 轨道\n",
      "Nearest to 汉中: 鞑靼 蒋村 于夫罗 道服 气化 留长 柴咲幸 王世杰\n",
      "Nearest to 而来: 各个方面 议席 爱因斯坦 名称 菁英 支助 地域 江左\n",
      "Nearest to 飞机: 严重 另外 长期 其后 高度 纽约 保持 引发\n",
      "Nearest to 大规模: 迅速 一系列 持续 长期 条件 直接 发生 完全\n",
      "Nearest to 诺贝尔奖: 蒙帕纳斯 每过 狩猎场 披风 大洋河 大自然 山地部落 编委会\n",
      "Nearest to 宁波: 穆德 女护士 儿茶酚胺 缺钱 左至 斯拉夫 朝阳门外 发烧友\n",
      "Nearest to 做法: 扩张 强烈 保障 这项 社会 职业 参与 消灭\n",
      "Nearest to 福建: 四川 以后 居住 主要 最后 广东 云南 发动\n",
      "Nearest to 收看: 常见病 横切面 行装 默写 贞通 铃置 社及 罗伯兹\n",
      "Nearest to 付费: 探病 哈利波 起才 浙菜 诸地 贝理雅 尤利西斯 劣质\n",
      "Nearest to 科举: 出于 石库门 政府职能 上层建筑 负离子 情况危急 华法林 检票\n",
      "Nearest to 撰写: 文章 就是 很多 之前 以后 往往 得以 一次\n",
      "Nearest to 距离: 相对 太阳 比较 地球 大约 观测 轨道 处于\n",
      "Nearest to 正确: 领域 那些 指出 相关 基于 只有 这些 可以\n",
      "Nearest to 商务: 同位 六八 王荷波 茶余饭后 上供 显影液 清溪镇 亚克兴\n",
      "Nearest to 透明: 北进 克拉伦斯 动态 主礼 渡海 陪都 马王堆 优先权\n",
      "Nearest to 乘积: 栩栩如生 中央美术学院 加略 回本 哈芬市 流动人口 史家 黄镇球\n",
      "Nearest to 山脉: 东部 海拔 北部 南部 西部 平原 西南 西北\n",
      "Nearest to 说法: 不过 认为 进一步 经常 现在 人们 过去 有关\n",
      "Nearest to 以色列: 犹太人 基督教 重新 保持 批评 世纪 直接 移民\n",
      "Nearest to 亿港元: 镜湖 阿尼哥 万新 辖县 西河 胁迫 快歌 无几\n",
      "Nearest to 共济会: 基本 革命 冲突 一定 避免 有效 宣布 强调\n",
      "Nearest to 原创: 喜子 华东理工大学 联军部队 眺望 接力赛 桐子 底版 灌婴\n",
      "Nearest to 总督: 正式 实行 规定 政权 回到 再次 要求 政府\n",
      "Nearest to 圣徒: 手令 居上 菜蔬 成虫 葛莱美 食尸 近处 闸口\n",
      "Nearest to 法兰西: 尸检 事业 教义 鲨堡 下丘脑 珠海国际赛车场 台面 伊斯兰教\n",
      "Nearest to 变法: 变时 抱头痛哭 梁惠王 噶玛兰族 制刀 很易 细粉 乡镇\n",
      "Nearest to 模型: 用来 简单 行为 不同 功能 经典 分析 基本\n",
      "Nearest to 年前: 统计 所以 以来 因而 目前 独立 停止 年底\n",
      "Nearest to 字节: 军政大学 回文 抛物线 好利 排字 马丁尼 示范作用 遗风\n",
      "Nearest to 福音: 葛罗 右昌 影视作品 提名权 隶属于 樱花大战 茶类 瑞士军刀\n",
      "Average loss at 52000 is 14.347467\n",
      "Average loss at 56000 is 13.897021\n",
      "Average loss at 60000 is 14.215495\n",
      "Average loss at 64000 is 13.875182\n",
      "Average loss at 68000 is 13.678959\n",
      "Average loss at 72000 is 13.965550\n",
      "Average loss at 76000 is 13.372301\n",
      "Average loss at 80000 is 13.110021\n",
      "Average loss at 84000 is 13.081313\n",
      "Average loss at 88000 is 12.926380\n",
      "Average loss at 92000 is 12.812818\n",
      "Average loss at 96000 is 12.936311\n",
      "Average loss at 100000 is 12.525827\n",
      "training time: 914.766038\n",
      "Nearest to 发射: 卫星 商业 达到 地面 目前 国际 目标 提升\n",
      "Nearest to 汉中: 之战 讨伐 比较 进攻 率军 率领 孙策 平定\n",
      "Nearest to 而来: 源自 重要 相关 宋代 学说 比较 行政区划 德语\n",
      "Nearest to 飞机: 目标 持续 空军 地面 成功 失败 无法 保护\n",
      "Nearest to 大规模: 及其 迅速 各国 此时 展开 爆发 逐渐 军队\n",
      "Nearest to 诺贝尔奖: 地铁 澳大利亚 成员国 车辆 地域 基地 生涯 湖北\n",
      "Nearest to 宁波: 众多 一座 杭州 交流 各地 十分 铁路 规划\n",
      "Nearest to 做法: 不能 没有 不过 原本 那些 明确 思想 做出\n",
      "Nearest to 福建: 沿海 广西 广东 东南 江西 浙江 湖南 云南\n",
      "Nearest to 收看: 横切面 常见病 罗伯兹 群是 语境 画上 偶像剧 行装\n",
      "Nearest to 付费: 贝理雅 探病 儒法 起才 通匪 此种 阳历 浙菜\n",
      "Nearest to 科举: 出于 还要 南京市 死刑 近年 纳入 同性 男性\n",
      "Nearest to 撰写: 外国 并未 原本 共同 形象 各国 提到 内容\n",
      "Nearest to 距离: 并且 位置 范围 接近 大小 之间 周围 延伸\n",
      "Nearest to 正确: 预测 能够 证据 其它 容易 相信 类似 观察\n",
      "Nearest to 商务: 同位 王荷波 显影液 东宫 反射层 人事处 六八 亚克兴\n",
      "Nearest to 透明: 后续 内核 动态 克拉伦斯 北进 渡海 解剖学 陪都\n",
      "Nearest to 乘积: 等于 流动人口 表述 栩栩如生 加略 几何 阻抗 近义\n",
      "Nearest to 山脉: 中部 西部 地形 平原 海拔 东部 南部 北部\n",
      "Nearest to 说法: 形象 加以 尤其 之中 而且 然而 这是 大致\n",
      "Nearest to 以色列: 入侵 外国 为此 阿拉伯 为了 一度 初期 冲突\n",
      "Nearest to 亿港元: 认识论 辖县 杂糅 镜湖 阿尼哥 催化氢化 马礼逊 万新\n",
      "Nearest to 共济会: 希望 政策 平等 不再 长期 生命 个人 这次\n",
      "Nearest to 原创: 喜子 演唱 水袖 乐章 底版 金像奖 生物学家 奖项\n",
      "Nearest to 总督: 实施 名义 随后 颁布 官员 一次 及其 行政\n",
      "Nearest to 圣徒: 成虫 菜蔬 手令 居上 吴士 食尸 葛莱美 近处\n",
      "Nearest to 法兰西: 统治 征服 贵族 入侵 拿破仑 王国 平民 国王\n",
      "Nearest to 变法: 儒家 推行 逐渐 渐渐 人民 西藏 体制 王安石\n",
      "Nearest to 模型: 简单 结构 类似 应用 计算 机制 工具 正确\n",
      "Nearest to 年前: 尤其 每年 已有 相当 处于 达到 提升 至今\n",
      "Nearest to 字节: 编码 逻辑 静态 命题 空间 一般 适用 转换\n",
      "Nearest to 福音: 右昌 法学院 提名权 隶属于 瑞士军刀 监督 组诗 影视作品\n",
      "Average loss at 104000 is 12.676849\n",
      "Average loss at 108000 is 12.517093\n",
      "Average loss at 112000 is 12.377427\n",
      "Average loss at 116000 is 12.112998\n",
      "Average loss at 120000 is 12.041787\n",
      "Average loss at 124000 is 11.973446\n",
      "Average loss at 128000 is 11.725835\n",
      "Average loss at 132000 is 11.881720\n",
      "Average loss at 136000 is 11.736749\n",
      "Average loss at 140000 is 11.881583\n",
      "Average loss at 144000 is 11.626121\n",
      "Average loss at 148000 is 11.553311\n",
      "training time: 913.280191\n",
      "Nearest to 发射: 并且 非常 设备 地面 能够 信号 它们 同样\n",
      "Nearest to 汉中: 洛阳 南下 攻打 率军 大败 全境 北伐 投降\n",
      "Nearest to 而来: 起源 常常 并非 学说 尤其 是从 特征 所谓\n",
      "Nearest to 飞机: 飞行 武器 持续 装备 制造 数量 投入 全部\n",
      "Nearest to 大规模: 得以 日后 继续 大批 纷纷 重新 遭受 本土\n",
      "Nearest to 诺贝尔奖: 瑞士 总部 各国 贡献 最多 国际 以来 尊重\n",
      "Nearest to 宁波: 成都 之一 城区 另有 杭州 修建 建成 周边\n",
      "Nearest to 做法: 时常 出来 有人 虽然 其实 为了 因而 所以\n",
      "Nearest to 福建: 广东 广西 江西 浙江 安徽 一带 沿海 云南\n",
      "Nearest to 收看: 常见病 横切面 上分 好几年 群是 虽属 语境 罗伯兹\n",
      "Nearest to 付费: 旅行者 贝理雅 此种 通匪 儒法 探病 浙菜 近似于\n",
      "Nearest to 科举: 经历 引进 近年 宋朝 农民 各省 长期 江南\n",
      "Nearest to 撰写: 一份 介绍 写作 提及 有关 评论 总结 发表\n",
      "Nearest to 距离: 测量 高度 之间 大小 接近 速度 复杂 精确\n",
      "Nearest to 正确: 错误 表达 相信 出来 那些 来说 事实 任何\n",
      "Nearest to 商务: 同位 英国政府 运输 王荷波 高清 配比 显影液 六八\n",
      "Nearest to 透明: 数据 实际上 深入 某个 有限 假设 常见 转换\n",
      "Nearest to 乘积: 平方 阻抗 流动人口 临近 栩栩如生 任意 生物学 唐诗\n",
      "Nearest to 山脉: 西部 中部 海拔 山地 地势 南部 平原 东部\n",
      "Nearest to 说法: 留下 至于 时候 过去 朋友 尽管 本人 看法\n",
      "Nearest to 以色列: 大批 日后 为此 直到 从未 受到 继续 声称\n",
      "Nearest to 亿港元: 杂糅 马礼逊 收入 苏贞昌 旧金山 麦克白 患病 联手\n",
      "Nearest to 共济会: 主导 一同 带领 群众 一年 仍然 不满 责任\n",
      "Nearest to 原创: 资讯 电台 奖项 创作 歌曲 声音 撰写 制作\n",
      "Nearest to 总督: 任命 颁布 随即 推翻 解散 为首 遭到 大臣\n",
      "Nearest to 圣徒: 派别 遵循 耶稣基督 血统 教会 纽约州 曼谷 多名\n",
      "Nearest to 法兰西: 拿破仑 领土 帝国 失败 第一次世界大战 伊朗 军官 建国\n",
      "Nearest to 变法: 秦国 建国 政权 体制 有所 确立 战国 消灭\n",
      "Nearest to 模型: 计算机 假设 描述 广义 物理 原理 对应 简单\n",
      "Nearest to 年前: 世纪末 已有 大约 过去 最早 至少 如今 历史\n",
      "Nearest to 字节: 编码 运算 静态 命题 流通 驾着 架构 逻辑\n",
      "Nearest to 福音: 基督教 上帝 圣经 基督 提及 宣称 天主教 许多\n",
      "Average loss at 152000 is 11.533983\n",
      "Average loss at 156000 is 11.665361\n",
      "Average loss at 160000 is 11.685117\n",
      "Average loss at 164000 is 11.289072\n",
      "Average loss at 168000 is 11.446120\n",
      "Average loss at 172000 is 11.258696\n",
      "Average loss at 176000 is 11.317001\n",
      "Average loss at 180000 is 11.448201\n",
      "Average loss at 184000 is 11.446189\n",
      "Average loss at 188000 is 11.328162\n",
      "Average loss at 192000 is 11.196452\n",
      "Average loss at 196000 is 11.138788\n",
      "Average loss at 200000 is 10.938119\n",
      "training time: 914.188883\n",
      "Nearest to 发射: 只能 只有 从而 爆炸 需要 不会 容易 携带\n",
      "Nearest to 汉中: 攻克 率军 南下 大败 攻下 江南 诸葛亮 黄河\n",
      "Nearest to 而来: 相当 提及 下来 所谓 极为 起源 十分 同一\n",
      "Nearest to 飞机: 空中 大型 飞行 空军 战斗机 不足 投入 派出\n",
      "Nearest to 大规模: 此后 即将 相继 大批 终于 几年 纷纷 逐步\n",
      "Nearest to 诺贝尔奖: 得主 颁发 杰出 物理学家 校友 获得 学会 科学家\n",
      "Nearest to 宁波: 杭州 浙江 隶属于 广东 建成 改为 城区 铁路\n",
      "Nearest to 做法: 为了 充满 感到 事实 采取 本人 不要 原因\n",
      "Nearest to 福建: 安徽 湖北 广东 湖南 浙江 云南 江苏 一带\n",
      "Nearest to 收看: 常见病 金曲 横切面 播出 国旗 满怀 上分 虽属\n",
      "Nearest to 付费: 旅行者 潜艇 卫星 贝理雅 海王星 客户端 主机 经济学家\n",
      "Nearest to 科举: 至此 元朝 家庭 雍正 鼓励 以后 六年 采纳\n",
      "Nearest to 撰写: 一书 出版 人物 以此 小说 里面 直到 描写\n",
      "Nearest to 距离: 两个 方向 一点 大小 固定 一个 构成 其中\n",
      "Nearest to 正确: 能够 出来 如何 这样 解释 清楚 适当 其实\n",
      "Nearest to 商务: 纽约 日起 为止 加入 居民 乘客 部份 进行\n",
      "Nearest to 透明: 玻璃 其他 特殊 很少 或者 可能 接近 区分\n",
      "Nearest to 乘积: 那么 集合 某个 得出 实数 整数 方程 平方\n",
      "Nearest to 山脉: 山地 平原 海拔 盆地 地势 南部 东南 海岸\n",
      "Nearest to 说法: 一词 提到 祖先 源自 古老 后世 意思 所谓\n",
      "Nearest to 以色列: 努力 此后 巴勒斯坦 国外 再度 这次 短暂 暂时\n",
      "Nearest to 亿港元: 收购 亿美元 旗下 集团 月底 新加坡 北美 投资\n",
      "Nearest to 共济会: 主导 宣告 名义 参与 秘密 终于 短暂 采取\n",
      "Nearest to 原创: 歌曲 奖项 电影 电视剧 演唱 舞台 香港电影 受欢迎\n",
      "Nearest to 总督: 任命 就任 官员 任职 投降 香港 军人 出任\n",
      "Nearest to 圣徒: 耶稣基督 派别 血统 传教 文艺复兴 义大利 教会 圣经\n",
      "Nearest to 法兰西: 王室 妻子 王位 争夺 意大利 打败 王国 伯爵\n",
      "Nearest to 变法: 制度 治理 局势 政权 秦国 政策 重视 巩固\n",
      "Nearest to 模型: 分析 应用 市场 描述 这个 符合 精确 用来\n",
      "Nearest to 年前: 已有 大部分 另有 来自 几年 继续 原本 今日\n",
      "Nearest to 字节: 编码 图形 某种 代码 列出 详细 分类 比较\n",
      "Nearest to 福音: 基督徒 圣经 上帝 信徒 诞生 基督 教义 权威\n",
      "Average loss at 204000 is 11.419649\n",
      "Average loss at 208000 is 10.951420\n",
      "Average loss at 212000 is 11.165521\n",
      "Average loss at 216000 is 11.040976\n",
      "Average loss at 220000 is 10.952866\n",
      "Average loss at 224000 is 10.795468\n",
      "Average loss at 228000 is 10.837609\n",
      "Average loss at 232000 is 10.499060\n",
      "Average loss at 236000 is 10.853468\n",
      "Average loss at 240000 is 10.876855\n",
      "Average loss at 244000 is 10.825794\n",
      "Average loss at 248000 is 10.890353\n",
      "training time: 911.949400\n",
      "Nearest to 发射: 火箭 轨道 探测 太空 任务 飞行 损失 打开\n",
      "Nearest to 汉中: 攻克 率军 诸葛亮 太守 荆州 大军 攻下 起兵\n",
      "Nearest to 而来: 起源 称之为 以前 源于 基本上 之意 而成 提到\n",
      "Nearest to 飞机: 飞行 空中 目标 战斗机 严重 武器 空军 损失\n",
      "Nearest to 大规模: 战后 逐步 军事 多次 配合 相继 大批 投入\n",
      "Nearest to 诺贝尔奖: 得主 校友 物理学家 杰出 颁发 贡献 芝加哥 教授\n",
      "Nearest to 宁波: 浙江省 杭州 厦门 大连 浙江 次年 更名 同年\n",
      "Nearest to 做法: 当作 与其 缺乏 视为 另一方面 一旦 即使 想法\n",
      "Nearest to 福建: 东南亚 大多 华侨 广东 来自 富有 聚居 现今\n",
      "Nearest to 收看: 无线 播放 电视台 节目 播出 电视节目 有线电视 电台\n",
      "Nearest to 付费: 研发 同步 无线 相关 旅行者 此类 厂商 网络\n",
      "Nearest to 科举: 十一年 读书 进士 康熙 八年 清末 明代 元代\n",
      "Nearest to 撰写: 编写 著作 评论 编辑 一本 文章 写道 一篇\n",
      "Nearest to 距离: 太阳 接近 地球 环绕 估计 半径 靠近 辐射\n",
      "Nearest to 正确: 确实 为何 想法 注意 本质 这样 判断 无论\n",
      "Nearest to 商务: 机构 临时 对外 日起 企业 竞争 更换 专用\n",
      "Nearest to 透明: 呈现 造成 结构 或是 不会 这种 原理 环境\n",
      "Nearest to 乘积: 整数 任意 公式 实数 某个 自然数 矩阵 给定\n",
      "Nearest to 山脉: 东南 地势 丘陵 地带 山地 平原 高原 地形\n",
      "Nearest to 说法: 提到 自称 有名 反而 指出 可见 出于 一词\n",
      "Nearest to 以色列: 巴勒斯坦 耶路撒冷 摧毁 该国 民众 确认 保持 纪念\n",
      "Nearest to 亿港元: 资产 出售 收购 股份 亿美元 持有 业务 截至\n",
      "Nearest to 共济会: 身分 争取 关注 日后 手中 另一方面 试图 未能\n",
      "Nearest to 原创: 剧本 作品 制作 题材 手法 创作 全新 电影\n",
      "Nearest to 总督: 相继 军人 官员 驱逐 随即 不久 灭亡 围攻\n",
      "Nearest to 圣徒: 信徒 耶稣基督 派别 不列颠 文艺复兴 修道院 使徒 源自\n",
      "Nearest to 法兰西: 查理 伯爵 路易 拿破仑 王位 王室 统治者 国王\n",
      "Nearest to 变法: 秦国 梁启超 战国 精神 地位 晚年 注重 纷纷\n",
      "Nearest to 模型: 改进 用来 假设 原理 模拟 应用 精确 参数\n",
      "Nearest to 年前: 多年 全世界 世界各地 来自 一直 有名 现今 短暂\n",
      "Nearest to 字节: 图形 符号 列出 编码 简单 功能 下面 逻辑\n",
      "Nearest to 福音: 基督 基督徒 基督教 新教 教义 耶稣 信徒 教会\n",
      "Average loss at 252000 is 10.867581\n",
      "Average loss at 256000 is 10.688342\n",
      "Average loss at 260000 is 10.849045\n",
      "Average loss at 264000 is 10.540898\n",
      "Average loss at 268000 is 10.712822\n",
      "Average loss at 272000 is 10.725844\n",
      "Average loss at 276000 is 10.528276\n",
      "Average loss at 280000 is 10.625477\n",
      "Average loss at 284000 is 10.546262\n",
      "Average loss at 288000 is 10.379129\n",
      "Average loss at 292000 is 10.467171\n",
      "Average loss at 296000 is 10.444678\n",
      "Average loss at 300000 is 10.362807\n",
      "training time: 913.098286\n",
      "Nearest to 发射: 火箭 地面 仪器 飞行 携带 探测 雷达 更大\n",
      "Nearest to 汉中: 上游 西南 全境 太守 以西 南下 南阳 东汉\n",
      "Nearest to 而来: 已有 带有 大多数 各种 之中 有所 以前 例外\n",
      "Nearest to 飞机: 起飞 一架 降落 飞行 客机 飞行员 改装 空中\n",
      "Nearest to 大规模: 迅速 冲突 展开 扩大 这次 发起 局势 不断\n",
      "Nearest to 诺贝尔奖: 得主 校友 杰出 年度 专业 颁发 培养 博士\n",
      "Nearest to 宁波: 杭州 厦门 浙江省 苏州 浙江 长江 迁至 历史悠久\n",
      "Nearest to 做法: 加以 形容 在于 有关 如此 并非 即使 指出\n",
      "Nearest to 福建: 福州 广西 广东 浙江 湖北 江苏 祖籍 湖南\n",
      "Nearest to 收看: 节目 广播 有线电视 播放 电视 无线 播出 频道\n",
      "Nearest to 付费: 免费 互联网 研发 软体 自动 平台 额外 网路\n",
      "Nearest to 科举: 十年 进士 五月 六年 朝廷 三年 康熙 战乱\n",
      "Nearest to 撰写: 编辑 介绍 一篇 一本 一书 肯定 发表 一份\n",
      "Nearest to 距离: 直线 长度 等于 假设 高度 方向 两条 其中\n",
      "Nearest to 正确: 理解 注意 只要 判断 某些 的话 选择 用来\n",
      "Nearest to 商务: 因应 率先 陆续 事务 初期 海外 管理 顾问\n",
      "Nearest to 透明: 呈现 即可 用来 多种 结构 参考 少量 外观\n",
      "Nearest to 乘积: 整数 任意 给出 自然数 给定 有限 小于 定理\n",
      "Nearest to 山脉: 平原 地带 地势 高原 地形 山区 海岸 地处\n",
      "Nearest to 说法: 有人 写下 形容 之下 早已 面前 然而 并未\n",
      "Nearest to 以色列: 埃及 叙利亚 巴勒斯坦 基督徒 统治 犹太人 奴隶 仍然\n",
      "Nearest to 亿港元: 收购 股权 股东 万美元 出售 亿美元 持有 旗下\n",
      "Nearest to 共济会: 一名 不过 一位 身份 希望 高层 宣称 几年\n",
      "Nearest to 原创: 剧情 配音 代表作 作品 拍摄 演员 爱情 动画\n",
      "Nearest to 总督: 任命 爵士 出任 接替 委任 任职 英国 上任\n",
      "Nearest to 圣徒: 圣经 耶稣基督 恐惧 犹太 基督徒 使徒 信徒 派别\n",
      "Nearest to 法兰西: 第一次世界大战 拿破仑 帝国 法国 脱离 伯爵 三世 王室\n",
      "Nearest to 变法: 一书 末期 以后 衰落 王安石 出兵 长期 国力\n",
      "Nearest to 模型: 精确 简单 理论 用来 标准 给出 参数 方法\n",
      "Nearest to 年前: 乃至 以来 频繁 地点 绝大部分 一部分 数个 广大\n",
      "Nearest to 字节: 字符 储存 固定 几种 序列 转换 标准 图形\n",
      "Nearest to 福音: 基督徒 耶稣 教派 圣经 基督 教会 天主教 信徒\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocabulary_size = 150000\n",
    "\n",
    "# with open('./data.txt', 'w+') as f:\n",
    "#     f.write(str(words[:800]))\n",
    "\n",
    "\n",
    "def bulid_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # Counter统计字符出现的个数，返回无序的dict，key为词、value为数；\n",
    "    # most_common会返回列表，且列表每一个元素为元组（key, value）\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count +=1\n",
    "        data.append(index)\n",
    "    # 将前面预设的「-1」替换为真正的集外词\n",
    "    count[0][1] = unk_count\n",
    "    # value -> key\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = bulid_dataset(words)\n",
    "del words\n",
    "print('Most common words(+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_windows):\n",
    "    \"\"\"\n",
    "    num_skip:每个单词生成的样本数；skip_windows:单词最远联系的距离\n",
    "    样本：（word1, word2_nearby_word1）\n",
    "    batch_size/num_skips = num_words\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_windows\n",
    "    batch = np.zeros(shape=[batch_size], dtype=np.int32)\n",
    "    labels = np.zeros([batch_size, 1], np.int32)\n",
    "    span = 2 * skip_windows + 1\n",
    "    # deque为双向队列，使用append方法只会保留后插入的span个变量\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    # span个值依次读入buffer\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        # 为一个批量的词生成样本\n",
    "        target = skip_windows\n",
    "        target_to_avoid = [skip_windows]\n",
    "        # 每个词生成num_skips个样本\n",
    "        for j in range(num_skips):\n",
    "            # 只要是已生成过的，就重新生成\n",
    "            while target in target_to_avoid:\n",
    "                # 大于等于0，小于等于span - 1\n",
    "                target = random.randint(0, span - 1)\n",
    "            target_to_avoid.append(target)\n",
    "            # 作为输入的词，每个词重复num_skips次\n",
    "            batch[i * num_skips + j] = buffer[skip_windows]\n",
    "            # 作为标注的词，输入词的左右\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        # 读取下一个词，丢弃buffer第一个词\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "# batch, labels = generate_batch(batch_size=8, num_skips=2, skip_windows=1)\n",
    "# for i in range(8):\n",
    "#     print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "batch_size = 512\n",
    "embedding_size = 300\n",
    "skip_window = 8\n",
    "num_skips = 16\n",
    "\n",
    "valid_size = 32\n",
    "valid_window = 10000\n",
    "# a=常数表示从零到常数抽取，a=数列则抽取元素；一次抽取size个元素。\n",
    "valid_examples = np.random.choice(a=valid_window, size=valid_size, replace=False)\n",
    "num_sampled = 5000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_inputs = tf.constant(valid_examples, tf.int32)\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        embeddings = tf.get_variable('embeddings', shape=[vocabulary_size, embedding_size],\n",
    "                                     initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        # 查找train_inputs对应的词嵌入向量\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        loss_weights = tf.get_variable('loss_weights', [vocabulary_size, embedding_size],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=1.0))\n",
    "        loss_biases = tf.get_variable('loss_bias', [vocabulary_size], initializer=tf.constant_initializer(0))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=loss_weights,\n",
    "                                         biases=loss_biases,\n",
    "                                         labels=train_labels,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=num_sampled,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "#     optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "#     optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_examples)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "num_steps = 300001\n",
    "start = time.time()\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 4000 == 0 and step > 0:\n",
    "            average_loss /= 4000\n",
    "            print('Average loss at %d is %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % 50000 == 0:\n",
    "            print('training time: %f' % (time.time()-start))\n",
    "            start = time.time()\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gj2HAOq8ySNF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SkipGram_CN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
