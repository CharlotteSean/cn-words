{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wpr9W4W4n6Z"
   },
   "source": [
    "### 下载维基中文语料并训练简单的词嵌入<br />\n",
    "主要步骤\n",
    "- 下载维基中文语料\n",
    "- 抽取中文文本\n",
    "- 转换繁体为简体\n",
    "- 训练词嵌入<br />\n",
    "\n",
    "需要安装的库\n",
    "- jieba分词\n",
    "- wikiextractor 抽取中文文本\n",
    "- opencc-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uU5666Bb4nSc"
   },
   "outputs": [],
   "source": [
    "# 下载维基数据\n",
    "! wget -c https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z6gPDMZf4tRV"
   },
   "outputs": [],
   "source": [
    "# 安装抽取和转换工具\n",
    "! pip install jieba\n",
    "! git clone https://github.com/attardi/wikiextractor.git\n",
    "! cd wikiextractor && python setup.py install\n",
    "# MacOS 可直接用Homebrew装，其它系统可查看原项目https://github.com/BYVoid/OpenCC\n",
    "! brew install OpenCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iTh2tQRS4nfg"
   },
   "outputs": [],
   "source": [
    "# 在wikiextractor目录下抽取维基文字，http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n",
    "! cd wikiextractor && python WikiExtractor.py -b 500M -o ../ ../zhwiki-latest-pages-articles1.xml-p1p162886.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P50OxYJn4xXT"
   },
   "outputs": [],
   "source": [
    "# 使用opencc 将繁体转换为简体\n",
    "! opencc -i input_filename -o output_filename -c t2s.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbKyFmQN46MT"
   },
   "source": [
    "## 部分文本演示\n",
    "\n",
    "也可以直接下载已经转换好的部分文本（180MB/1.6GB）：https://drive.google.com/open?id=1ORNDviCeIIiosE_XlEEJfJHp0F01mtsQ\n",
    "\n",
    "下面以这部分文本做演示：[Colab地址](https://colab.research.google.com/drive/1uFnqsHyIn5C84pVkYW_dpoGB9NG7eFme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YiWOJ7Cc4xa1"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive \n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WiQGDRk14xeL"
   },
   "outputs": [],
   "source": [
    "extraed = '1ORNDviCeIIiosE_XlEEJfJHp0F01mtsQ'\n",
    "txt = drive.CreateFile({'id':extraed})\n",
    "txt.GetContentFile('wiki_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "hDWA-MRl5RLr",
    "outputId": "b46ce464-8c6a-4040-e4fb-0d914f525917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.39)\n",
      "--2018-12-15 09:19:30--  http://horatio-jsy.oss-cn-beijing.aliyuncs.com/seg_dict.txt\n",
      "Resolving horatio-jsy.oss-cn-beijing.aliyuncs.com (horatio-jsy.oss-cn-beijing.aliyuncs.com)... 59.110.185.122\n",
      "Connecting to horatio-jsy.oss-cn-beijing.aliyuncs.com (horatio-jsy.oss-cn-beijing.aliyuncs.com)|59.110.185.122|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11167 (11K) [text/plain]\n",
      "Saving to: ‘seg_dict.txt’\n",
      "\n",
      "seg_dict.txt        100%[===================>]  10.91K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-15 09:19:31 (165 MB/s) - ‘seg_dict.txt’ saved [11167/11167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install jieba\n",
    "! wget -c http://horatio-jsy.oss-cn-beijing.aliyuncs.com/seg_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "--kPDPSc5RWU",
    "outputId": "fd40a280-35e8-45e0-8056-3e36e7b0ee31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.043 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgement time: 344.09616708755493\n",
      "total number of words: 37541365\n",
      "total number of words after preprocess: 17703257\n",
      "data size 17703257\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "with open('./wiki_text.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read()\n",
    "jieba.load_userdict(\"./seg_dict.txt\")\n",
    "\n",
    "def read_data(txt):\n",
    "    seg_list = []\n",
    "    start = time.time()\n",
    "    cut_list = jieba.lcut(txt)\n",
    "    print('sgement time:', time.time()-start)\n",
    "    print('total number of words:', len(cut_list))\n",
    "\n",
    "    except_no = re.compile(r'[\\u4e00-\\u9fa5]{2,}')\n",
    "    for i in cut_list:\n",
    "        if except_no.search(i) is not None:\n",
    "            seg_list.append(i)\n",
    "    print('total number of words after preprocess:', len(seg_list))\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "# 返回的words是一个列表，每一个元素是一个单词字符\n",
    "words = read_data(txt)\n",
    "print('data size', len(words))\n",
    "\n",
    "# 节省分词的时间\n",
    "with open('words_data_saving.p', 'wb') as f:\n",
    "    pickle.dump(words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7721
    },
    "colab_type": "code",
    "id": "h6I4ZvNl5Rci",
    "outputId": "5a1f130f-95e8-4949-d416-2bc48ba2f31e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelling time: 1.1920928955078125e-05\n",
      "WARNING:tensorflow:From <ipython-input-1-b878fc733840>:130: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "fit corpus time: 165.28611016273499\n",
      "Average loss at 5000 is 538.168232\n",
      "Average loss at 10000 is 393.484393\n",
      "Average loss at 15000 is 324.043227\n",
      "Average loss at 20000 is 279.659004\n",
      "Average loss at 25000 is 251.959477\n",
      "Average loss at 30000 is 225.221584\n",
      "Average loss at 35000 is 209.419566\n",
      "Average loss at 40000 is 192.015937\n",
      "Average loss at 45000 is 179.629480\n",
      "Average loss at 50000 is 167.615812\n",
      "Average loss at 55000 is 159.700317\n",
      "the training time in one epoch: 206.182901\n",
      "Nearest to 实际上: 情况 但是 一般 虽然 没有 矛盾 富庶 他们\n",
      "Nearest to 出任: 横轴 韬光 音讯 法子 画院 通海 加加林 硬朗\n",
      "Nearest to 网络: 香港 萨伏伊 玛格丽特 每隔 密奏 安插 富康 中国气象局\n",
      "Nearest to 正在: 解决方案 齐心 第九 飞机 爆炸性 主角奖 开始 其他\n",
      "Nearest to 能量: 松烟 本条 日生 转换 林旭 平衡态 光滑 廿六\n",
      "Nearest to 关系: 以及 一个 因此 然而 之后 他们 其他 可以\n",
      "Nearest to 知道: 因为 我们 当时 已经 他们 李光前 但是 魏博\n",
      "Nearest to 组成: 以及 包括 因此 一个 称为 部分 作为 由于\n",
      "Nearest to 电视台: 持家 康隆 上溯 自如 赈灾 主题 包公 庐江县\n",
      "Nearest to 四个: 其中 联大 一个 博德 因为 故须 三个 分别\n",
      "Nearest to 基督教: 数秒 犹太教 彰显 住院治疗 铁皮 服务端 殊异 玉雕\n",
      "Nearest to 少数: 使用 相对 名曲 甚至 策画 方式 线于 只有\n",
      "Nearest to 科学家: 多部 孑遗 盒装 共同 争路 数学界 花掉 丽水\n",
      "Nearest to 教授: 以及 包括 传统 其他 四方 科仪 指出 臧式毅\n",
      "Nearest to 最初: 由于 以及 包括 使用 许多 一个 当时 其他\n",
      "Nearest to 发现: 以及 中国 包括 他们 由于 一个 因此 没有\n",
      "Nearest to 两个: 一个 因此 以及 部分 由于 可以 然而 其他\n",
      "Nearest to 有效: 方法 基加利 先公 莫斯 人流量 可以 推力 请教\n",
      "Nearest to 实际: 控制 包括 因此 研究 他们 余公里 设立 部分\n",
      "Nearest to 东部: 全球 以及 巴赛族 母牛 花押 亲和力 最佳化 玛雅人\n",
      "Nearest to 土地: 变性人 只有 键盘输入 循规蹈矩 主修 部分 考虑 昌图县\n",
      "Nearest to 能力: 以及 因此 开始 一个 由于 称为 过程 部分\n",
      "Nearest to 帮助: 作为 一个 注意 以及 包括 因此 因为 出现\n",
      "Nearest to 交通: 透明 丰富 乡贤 商鞅 吕洞宾 罗巴 观战 跑道\n",
      "Nearest to 皇帝: 判刑 信誉 防疫 冲到 瓜氨酸 区位 三世 城是\n",
      "Nearest to 玩家: 六幅 助攻王 荒岛 炼油厂 法人资格 纸上 大型机 出卖\n",
      "Nearest to 进一步: 就是 就近 研究 其四 朋克 闰年 小食 优异成绩\n",
      "Nearest to 议会: 轻机枪 最佳影片 汴京 离石 特殊任务 假山 组分 敌对行动\n",
      "Nearest to 明显: 风笛 内江 可以 不同 体有 乌海市 但是 发展\n",
      "Nearest to 支援: 范围 为弱 口才 威风 无事 亚历克 掠过 因事\n",
      "Nearest to 兴建: 足球界 阿达马 田湾 国家体委 沙市 武田晴信 放款 招考\n",
      "Nearest to 取消: 主帅 电视剧 进行 闽海 多个 通顺 由于 雅集\n",
      "Average loss at 60000 is 131.196857\n",
      "Average loss at 65000 is 83.273903\n",
      "Average loss at 70000 is 79.760507\n",
      "Average loss at 75000 is 79.284473\n",
      "Average loss at 80000 is 75.231266\n",
      "Average loss at 85000 is 76.485353\n",
      "Average loss at 90000 is 73.101767\n",
      "Average loss at 95000 is 73.274244\n",
      "Average loss at 100000 is 70.245640\n",
      "Average loss at 105000 is 68.825178\n",
      "Average loss at 110000 is 68.080832\n",
      "Average loss at 115000 is 67.217934\n",
      "the training time in one epoch: 229.315798\n",
      "Average loss at 120000 is 50.927610\n",
      "Average loss at 125000 is 40.119045\n",
      "Average loss at 130000 is 39.846789\n",
      "Average loss at 135000 is 40.585834\n",
      "Average loss at 140000 is 39.812578\n",
      "Average loss at 145000 is 39.454390\n",
      "Average loss at 150000 is 38.964497\n",
      "Average loss at 155000 is 38.571094\n",
      "Average loss at 160000 is 38.330856\n",
      "Average loss at 165000 is 37.849828\n",
      "Average loss at 170000 is 37.156722\n",
      "Average loss at 175000 is 38.183468\n",
      "the training time in one epoch: 221.890797\n",
      "Nearest to 实际上: 但是 没有 就是 虽然 然而 因此 因为 不是\n",
      "Nearest to 出任: 担任 同年 上海 成立 随后 再次 宣布 先后\n",
      "Nearest to 网络: 香港 使用 以及 包括 拥有 区域 部分 利用\n",
      "Nearest to 正在: 开始 当时 部分 同时 由于 此外 现在 以及\n",
      "Nearest to 能量: 相同 产生 所以 松烟 本条 某些 受到 模型\n",
      "Nearest to 关系: 因此 因为 然而 一个 但是 他们 由于 称为\n",
      "Nearest to 知道: 我们 他们 自己 不是 因为 这样 但是 已经\n",
      "Nearest to 组成: 包括 部分 称为 以及 分别 一个 作为 其他\n",
      "Nearest to 电视台: 持家 主题 广东 康隆 上溯 包公 钢筋 资源配置\n",
      "Nearest to 四个: 三个 分别 两个 分为 其中 一个 不同 通常\n",
      "Nearest to 基督教: 传统 宗教 认为 自己 对于 相对 影响 称为\n",
      "Nearest to 少数: 只有 除了 使用 相对 其他 这些 甚至 一些\n",
      "Nearest to 科学家: 共同 认为 发现 成为 著名 时间 最初 后来\n",
      "Nearest to 教授: 研究 包括 学生 以及 传统 指出 主要 设计\n",
      "Nearest to 最初: 后来 由于 称为 以及 当时 包括 因此 一个\n",
      "Nearest to 发现: 因此 然而 由于 因为 但是 以及 包括 称为\n",
      "Nearest to 两个: 一个 因此 就是 称为 可以 因为 部分 分别\n",
      "Nearest to 有效: 方法 利用 可以 非常 所以 不能 不是 完全\n",
      "Nearest to 实际: 使得 他们 因此 而且 可以 但是 并且 因为\n",
      "Nearest to 东部: 北部 位于 南部 地区 西部 中部 附近 中国\n",
      "Nearest to 土地: 部分 加上 由于 包括 以及 同时 只有 然而\n",
      "Nearest to 能力: 因此 使得 并且 因为 由于 可以 称为 这些\n",
      "Nearest to 帮助: 并且 作为 因此 他们 同时 利用 因为 然而\n",
      "Nearest to 交通: 中心 建设 方式 地区 位于 中国 分别 丰富\n",
      "Nearest to 皇帝: 罗马 基本 父亲 此后 神圣 于是 判刑 三世\n",
      "Nearest to 玩家: 选择 自然 六幅 炼油厂 助攻王 试图 方式 工程\n",
      "Nearest to 进一步: 研究 发展 得到 并且 为了 就是 同时 进行\n",
      "Nearest to 议会: 通过 轻机枪 同年 实行 宣布 政府 中华人民共和国 一名\n",
      "Nearest to 明显: 然而 因此 可能 不同 但是 尤其 使得 而且\n",
      "Nearest to 支援: 范围 同时 方面 以及 技术 包括 进行 利用\n",
      "Nearest to 兴建: 分别 完成 香港 计划 一座 目前 包括 附近\n",
      "Nearest to 取消: 后来 最后 进行 之后 开始 最终 当时 决定\n",
      "Average loss at 180000 is 28.561803\n",
      "Average loss at 185000 is 27.269048\n",
      "Average loss at 190000 is 27.493453\n",
      "Average loss at 195000 is 27.202394\n",
      "Average loss at 200000 is 27.482960\n",
      "Average loss at 205000 is 27.302685\n",
      "Average loss at 210000 is 27.214532\n",
      "Average loss at 215000 is 26.930545\n",
      "Average loss at 220000 is 27.039862\n",
      "Average loss at 225000 is 26.850631\n",
      "Average loss at 230000 is 26.319275\n",
      "the training time in one epoch: 224.225271\n",
      "Average loss at 235000 is 25.605688\n",
      "Average loss at 240000 is 21.504911\n",
      "Average loss at 245000 is 21.527836\n",
      "Average loss at 250000 is 21.686467\n",
      "Average loss at 255000 is 21.480870\n",
      "Average loss at 260000 is 21.816165\n",
      "Average loss at 265000 is 21.529576\n",
      "Average loss at 270000 is 21.498204\n",
      "Average loss at 275000 is 21.295676\n",
      "Average loss at 280000 is 21.666445\n",
      "Average loss at 285000 is 21.428550\n",
      "Average loss at 290000 is 21.700564\n",
      "the training time in one epoch: 225.490324\n",
      "Nearest to 实际上: 就是 没有 不是 但是 因为 所以 因此 然而\n",
      "Nearest to 出任: 担任 同年 成立 上海 先后 任命 随后 宣布\n",
      "Nearest to 网络: 使用 系统 以及 香港 包括 其他 采用 例如\n",
      "Nearest to 正在: 开始 目前 同时 当时 现在 此外 进行 已经\n",
      "Nearest to 能量: 相同 产生 某些 或是 形式 所以 电子 一种\n",
      "Nearest to 关系: 因此 因为 但是 然而 并且 一个 之间 由于\n",
      "Nearest to 知道: 我们 应该 他们 自己 不是 这样 因为 如果\n",
      "Nearest to 组成: 包括 部分 称为 其他 以及 分别 一个 作为\n",
      "Nearest to 电视台: 广东 主题 持家 上溯 康隆 钢筋 包公 播放\n",
      "Nearest to 四个: 三个 两个 分为 分别 一个 其中 不同 部分\n",
      "Nearest to 基督教: 传统 宗教 早期 西方 视为 认为 影响 教会\n",
      "Nearest to 少数: 只有 除了 相对 其他 一些 部分 使用 这些\n",
      "Nearest to 科学家: 著名 发现 认为 后来 成为 不过 进入 当时\n",
      "Nearest to 教授: 学生 研究 担任 包括 之一 以及 方面 指出\n",
      "Nearest to 最初: 后来 由于 称为 同时 以及 作为 虽然 另外\n",
      "Nearest to 发现: 因此 但是 然而 因为 这些 称为 虽然 由于\n",
      "Nearest to 两个: 一个 就是 称为 因此 因为 例如 可以 所以\n",
      "Nearest to 有效: 方法 利用 无法 不能 能够 可以 得到 完全\n",
      "Nearest to 实际: 使得 而且 因此 并且 可以 他们 因为 但是\n",
      "Nearest to 东部: 北部 南部 西部 位于 地区 一带 中部 附近\n",
      "Nearest to 土地: 拥有 大部分 包括 大量 部分 由于 以及 当地\n",
      "Nearest to 能力: 使得 并且 因此 可以 这些 因为 所以 或者\n",
      "Nearest to 帮助: 并且 他们 同时 因此 因为 然而 已经 虽然\n",
      "Nearest to 交通: 中心 建设 地区 方面 位于 主要 发展 为主\n",
      "Nearest to 皇帝: 父亲 罗马 此后 建立 神圣 于是 去世 之后\n",
      "Nearest to 玩家: 选择 方式 部份 利用 进行 透过 自然 试图\n",
      "Nearest to 进一步: 得到 为了 发展 并且 研究 必须 因此 进行\n",
      "Nearest to 议会: 政府 宣布 人民 通过 实行 同年 中华人民共和国 决定\n",
      "Nearest to 明显: 可能 而且 影响 尤其 不同 导致 因此 使得\n",
      "Nearest to 支援: 提供 技术 全部 进行 同时 以及 方面 透过\n",
      "Nearest to 兴建: 完成 计划 香港 分别 建成 目前 附近 一座\n",
      "Nearest to 取消: 决定 之后 后来 随后 最终 最后 于是 宣布\n",
      "Average loss at 295000 is 19.961908\n",
      "Average loss at 300000 is 18.274922\n",
      "Average loss at 305000 is 18.175686\n",
      "Average loss at 310000 is 18.173613\n",
      "Average loss at 315000 is 18.356740\n",
      "Average loss at 320000 is 18.611705\n",
      "Average loss at 325000 is 18.268835\n",
      "Average loss at 330000 is 18.359727\n",
      "Average loss at 335000 is 18.308941\n",
      "Average loss at 340000 is 18.526494\n",
      "Average loss at 345000 is 18.402839\n",
      "Average loss at 350000 is 18.745573\n",
      "the training time in one epoch: 225.309287\n",
      "Average loss at 355000 is 16.640696\n",
      "Average loss at 360000 is 15.949657\n",
      "Average loss at 365000 is 16.159526\n",
      "Average loss at 370000 is 16.294578\n",
      "Average loss at 375000 is 16.227016\n",
      "Average loss at 380000 is 16.310605\n",
      "Average loss at 385000 is 16.366959\n",
      "Average loss at 390000 is 16.292280\n",
      "Average loss at 395000 is 16.559988\n",
      "Average loss at 400000 is 16.785587\n",
      "Average loss at 405000 is 16.600208\n",
      "the training time in one epoch: 224.326072\n",
      "Nearest to 实际上: 就是 但是 因为 没有 不是 所以 因此 然而\n",
      "Nearest to 出任: 担任 同年 任命 先后 成立 上海 宣布 随即\n",
      "Nearest to 网络: 系统 使用 信息 提供 相关 服务 以及 技术\n",
      "Nearest to 正在: 开始 目前 同时 已经 进行 此外 完成 现在\n",
      "Nearest to 能量: 相同 产生 某些 电子 释放 或是 形式 一种\n",
      "Nearest to 关系: 但是 因为 因此 并且 之间 所以 然而 而且\n",
      "Nearest to 知道: 我们 应该 自己 他们 这样 不是 人们 时候\n",
      "Nearest to 组成: 包括 其他 称为 部分 以及 分别 作为 一个\n",
      "Nearest to 电视台: 广东 播放 主题 制作 持家 南方 新闻 上溯\n",
      "Nearest to 四个: 三个 两个 分为 分别 一个 其中 不同 部分\n",
      "Nearest to 基督教: 传统 宗教 早期 西方 教会 视为 认为 尤其\n",
      "Nearest to 少数: 只有 其他 除了 一些 部分 大部分 大多数 许多\n",
      "Nearest to 科学家: 著名 发现 认为 后来 当时 其后 进入 成为\n",
      "Nearest to 教授: 担任 学生 研究 学校 该校 大学 包括 之一\n",
      "Nearest to 最初: 后来 作为 另外 称为 同时 由于 虽然 以及\n",
      "Nearest to 发现: 因此 但是 称为 虽然 这些 然而 因为 一个\n",
      "Nearest to 两个: 一个 称为 就是 这个 例如 所有 因此 所以\n",
      "Nearest to 有效: 无法 利用 能够 不能 方法 得到 完全 可以\n",
      "Nearest to 实际: 使得 而且 因此 并且 情况 所以 可以 对于\n",
      "Nearest to 东部: 北部 南部 西部 位于 中部 一带 地区 境内\n",
      "Nearest to 土地: 大部分 拥有 当地 大量 其中 分别 包括 部分\n",
      "Nearest to 能力: 使得 并且 因此 或者 可以 这些 所以 而且\n",
      "Nearest to 帮助: 他们 并且 因为 因此 同时 然而 自己 但是\n",
      "Nearest to 交通: 中心 建设 方面 为主 地区 位于 主要 重要\n",
      "Nearest to 皇帝: 父亲 罗马 此后 去世 建立 统治 不久 于是\n",
      "Nearest to 玩家: 选择 透过 允许 方式 利用 进行 能够 部份\n",
      "Nearest to 进一步: 得到 为了 发展 并且 必须 因此 进行 不断\n",
      "Nearest to 议会: 政府 人民 宣布 通过 实行 决定 代表 中华人民共和国\n",
      "Nearest to 明显: 可能 而且 影响 情况 比较 具有 这种 不同\n",
      "Nearest to 支援: 提供 技术 版本 全部 透过 进行 系统 采用\n",
      "Nearest to 兴建: 建成 计划 完成 香港 一座 建设 连接 目前\n",
      "Nearest to 取消: 决定 宣布 随后 之后 最终 于是 最后 其后\n",
      "Average loss at 410000 is 16.337721\n",
      "Average loss at 415000 is 14.464783\n",
      "Average loss at 420000 is 14.620218\n",
      "Average loss at 425000 is 14.614681\n",
      "Average loss at 430000 is 14.853376\n",
      "Average loss at 435000 is 15.038916\n",
      "Average loss at 440000 is 14.845402\n",
      "Average loss at 445000 is 14.724605\n",
      "Average loss at 450000 is 15.013034\n",
      "Average loss at 455000 is 15.031937\n",
      "Average loss at 460000 is 15.224018\n",
      "Average loss at 465000 is 15.227898\n",
      "the training time in one epoch: 200.118571\n",
      "Average loss at 470000 is 14.405986\n",
      "Average loss at 475000 is 13.562559\n",
      "Average loss at 480000 is 13.615026\n",
      "Average loss at 485000 is 13.596233\n",
      "Average loss at 490000 is 13.866263\n",
      "Average loss at 495000 is 13.857589\n",
      "Average loss at 500000 is 13.792517\n",
      "Average loss at 505000 is 13.791289\n",
      "Average loss at 510000 is 13.780236\n",
      "Average loss at 515000 is 13.863289\n",
      "Average loss at 520000 is 13.812431\n",
      "Average loss at 525000 is 14.211861\n",
      "the training time in one epoch: 224.112445\n",
      "Nearest to 实际上: 但是 就是 不是 因为 没有 所以 因此 这个\n",
      "Nearest to 出任: 担任 任命 同年 先后 邀请 成立 宣布 时任\n",
      "Nearest to 网络: 系统 信息 提供 服务 使用 相关 技术 用于\n",
      "Nearest to 正在: 开始 目前 完成 同时 已经 进行 计划 此外\n",
      "Nearest to 能量: 产生 释放 电子 相同 某些 或是 形式 一种\n",
      "Nearest to 关系: 因此 但是 因为 之间 并且 所以 而且 使得\n",
      "Nearest to 知道: 我们 应该 看到 这样 他们 自己 人们 不是\n",
      "Nearest to 组成: 包括 其他 称为 部分 分别 所有 一个 以及\n",
      "Nearest to 电视台: 广东 播放 制作 东京 南方 新闻 广播 播出\n",
      "Nearest to 四个: 三个 两个 分为 分别 一个 其中 部分 不同\n",
      "Nearest to 基督教: 传统 宗教 西方 早期 教会 视为 认为 文化\n",
      "Nearest to 少数: 只有 大多数 其他 大部分 除了 一些 部分 许多\n",
      "Nearest to 科学家: 著名 发现 认为 后来 曾经 指出 当时 许多\n",
      "Nearest to 教授: 担任 学生 学校 大学 该校 研究 学者 学院\n",
      "Nearest to 最初: 另外 作为 后来 称为 同时 虽然 除了 由于\n",
      "Nearest to 发现: 因此 但是 这些 因为 称为 虽然 不过 然而\n",
      "Nearest to 两个: 一个 就是 称为 所有 这个 例如 三个 另外\n",
      "Nearest to 有效: 无法 能够 不能 方法 利用 得到 完全 为了\n",
      "Nearest to 实际: 使得 因此 情况 而且 所以 并且 但是 因为\n",
      "Nearest to 东部: 北部 南部 西部 中部 位于 一带 地区 境内\n",
      "Nearest to 土地: 大部分 拥有 当地 大量 面积 其中 部分 包括\n",
      "Nearest to 能力: 并且 使得 可以 或者 因此 利用 而且 这些\n",
      "Nearest to 帮助: 他们 并且 得到 因为 然而 自己 但是 虽然\n",
      "Nearest to 交通: 中心 建设 方面 为主 重要 主要 发展 经济\n",
      "Nearest to 皇帝: 父亲 罗马 统治 去世 建立 此后 不久 国王\n",
      "Nearest to 玩家: 选择 允许 透过 方式 利用 能够 进行 直接\n",
      "Nearest to 进一步: 得到 为了 并且 从而 发展 不断 必须 因此\n",
      "Nearest to 议会: 政府 人民 实行 宣布 代表 通过 中华人民共和国 选举\n",
      "Nearest to 明显: 比较 而且 可能 情况 具有 影响 这种 相当\n",
      "Nearest to 支援: 提供 版本 技术 透过 系统 采用 全部 进行\n",
      "Nearest to 兴建: 建成 计划 建设 完成 连接 一座 香港 建造\n",
      "Nearest to 取消: 决定 宣布 随后 最终 之后 要求 于是 最后\n",
      "Average loss at 530000 is 13.016309\n",
      "Average loss at 535000 is 12.862340\n",
      "Average loss at 540000 is 12.732534\n",
      "Average loss at 545000 is 12.824523\n",
      "Average loss at 550000 is 12.761052\n",
      "Average loss at 555000 is 13.008368\n",
      "Average loss at 560000 is 12.897591\n",
      "Average loss at 565000 is 12.881237\n",
      "Average loss at 570000 is 13.117009\n",
      "Average loss at 575000 is 13.291100\n",
      "Average loss at 580000 is 13.136159\n",
      "Average loss at 585000 is 13.126830\n",
      "the training time in one epoch: 225.767550\n",
      "Average loss at 590000 is 12.006243\n",
      "Average loss at 595000 is 12.020279\n",
      "Average loss at 600000 is 12.091464\n",
      "Average loss at 605000 is 12.150912\n",
      "Average loss at 610000 is 12.348954\n",
      "Average loss at 615000 is 12.121797\n",
      "Average loss at 620000 is 12.255611\n",
      "Average loss at 625000 is 12.392848\n",
      "Average loss at 630000 is 12.321929\n",
      "Average loss at 635000 is 12.354878\n",
      "Average loss at 640000 is 12.452039\n",
      "the training time in one epoch: 225.644134\n",
      "Nearest to 实际上: 但是 就是 因为 不是 所以 没有 因此 这个\n",
      "Nearest to 出任: 担任 任命 同年 邀请 时任 先后 成立 主席\n",
      "Nearest to 网络: 信息 系统 提供 服务 相关 技术 使用 用于\n",
      "Nearest to 正在: 开始 完成 同时 已经 进行 计划 目前 并且\n",
      "Nearest to 能量: 产生 释放 电子 相同 某些 或是 它们 形式\n",
      "Nearest to 关系: 因此 因为 但是 之间 并且 所以 使得 而且\n",
      "Nearest to 知道: 我们 应该 看到 人们 只要 即使 自己 这样\n",
      "Nearest to 组成: 包括 其他 部分 分别 所有 称为 一个 以及\n",
      "Nearest to 电视台: 广东 播放 东京 制作 广播 南方 新闻 播出\n",
      "Nearest to 四个: 三个 两个 分为 分别 几个 一个 其中 部分\n",
      "Nearest to 基督教: 传统 宗教 西方 教会 早期 视为 民间 古代\n",
      "Nearest to 少数: 大多数 只有 大部分 其他 一些 部分 除了 以外\n",
      "Nearest to 科学家: 发现 著名 认为 后来 学者 指出 许多 曾经\n",
      "Nearest to 教授: 担任 学生 学校 大学 该校 研究 学习 学院\n",
      "Nearest to 最初: 另外 作为 同时 后来 称为 除了 虽然 由于\n",
      "Nearest to 发现: 因此 这些 但是 称为 因为 不过 虽然 然而\n",
      "Nearest to 两个: 一个 三个 称为 就是 所有 这个 例如 通常\n",
      "Nearest to 有效: 无法 能够 不能 利用 得到 方法 为了 需要\n",
      "Nearest to 实际: 使得 情况 因此 所以 并且 而且 但是 需要\n",
      "Nearest to 东部: 北部 南部 西部 中部 位于 一带 地区 境内\n",
      "Nearest to 土地: 大部分 拥有 面积 当地 大量 其中 部分 分别\n",
      "Nearest to 能力: 需要 并且 可以 利用 或者 使得 而且 因此\n",
      "Nearest to 帮助: 他们 并且 得到 自己 但是 因为 没有 为了\n",
      "Nearest to 交通: 中心 建设 方面 重要 为主 发展 主要 经济\n",
      "Nearest to 皇帝: 统治 罗马 父亲 去世 国王 不久 建立 神圣\n",
      "Nearest to 玩家: 选择 允许 透过 利用 方式 角色 能够 直接\n",
      "Nearest to 进一步: 为了 得到 从而 不断 并且 发展 必须 进行\n",
      "Nearest to 议会: 政府 人民 实行 选举 任命 宣布 代表 中华人民共和国\n",
      "Nearest to 明显: 比较 相当 而且 具有 可能 情况 一定 影响\n",
      "Nearest to 支援: 提供 版本 技术 系统 采用 开发 透过 功能\n",
      "Nearest to 兴建: 建成 建造 建设 计划 连接 一座 完成 香港\n",
      "Nearest to 取消: 决定 宣布 随后 要求 最终 之后 于是 其后\n",
      "Average loss at 645000 is 12.239847\n",
      "Average loss at 650000 is 11.454899\n",
      "Average loss at 655000 is 11.443332\n",
      "Average loss at 660000 is 11.452620\n",
      "Average loss at 665000 is 11.495222\n",
      "Average loss at 670000 is 11.681142\n",
      "Average loss at 675000 is 11.515919\n",
      "Average loss at 680000 is 11.751360\n",
      "Average loss at 685000 is 11.878444\n",
      "Average loss at 690000 is 11.861139\n",
      "Average loss at 695000 is 11.860817\n",
      "Average loss at 700000 is 11.913525\n",
      "the training time in one epoch: 225.204715\n",
      "Average loss at 705000 is 11.448090\n",
      "Average loss at 710000 is 10.951319\n",
      "Average loss at 715000 is 10.998616\n",
      "Average loss at 720000 is 11.181127\n",
      "Average loss at 725000 is 10.986424\n",
      "Average loss at 730000 is 10.960488\n",
      "Average loss at 735000 is 11.170856\n",
      "Average loss at 740000 is 11.308410\n",
      "Average loss at 745000 is 11.343283\n",
      "Average loss at 750000 is 11.223995\n",
      "Average loss at 755000 is 11.417932\n",
      "Average loss at 760000 is 11.571259\n",
      "the training time in one epoch: 222.309886\n",
      "Nearest to 实际上: 但是 不是 因为 就是 所以 没有 因此 这个\n",
      "Nearest to 出任: 担任 任命 同年 时任 邀请 先后 主席 成立\n",
      "Nearest to 网络: 信息 系统 服务 提供 相关 技术 使用 用于\n",
      "Nearest to 正在: 开始 完成 计划 同时 已经 目前 进行 并且\n",
      "Nearest to 能量: 释放 产生 电子 相同 它们 某些 或是 吸收\n",
      "Nearest to 关系: 因此 因为 但是 之间 使得 并且 所以 而且\n",
      "Nearest to 知道: 我们 应该 看到 只要 人们 那些 自己 即使\n",
      "Nearest to 组成: 包括 所有 其他 部分 分别 称为 一个 以及\n",
      "Nearest to 电视台: 广东 播放 东京 广播 南方 播出 新闻 制作\n",
      "Nearest to 四个: 三个 分为 两个 分别 几个 一个 其中 部分\n",
      "Nearest to 基督教: 传统 宗教 西方 教会 早期 视为 信仰 民间\n",
      "Nearest to 少数: 大多数 大部分 其他 只有 以外 一些 除了 部分\n",
      "Nearest to 科学家: 发现 认为 著名 学者 指出 后来 许多 大多数\n",
      "Nearest to 教授: 担任 学校 大学 学生 该校 学习 研究 学院\n",
      "Nearest to 最初: 另外 作为 后来 称为 同时 除了 虽然 此外\n",
      "Nearest to 发现: 但是 因此 称为 这些 虽然 因为 不过 然而\n",
      "Nearest to 两个: 一个 三个 称为 所有 就是 这个 例如 分别\n",
      "Nearest to 有效: 能够 无法 不能 得到 利用 为了 需要 避免\n",
      "Nearest to 实际: 使得 情况 因此 并且 所以 而且 但是 需要\n",
      "Nearest to 东部: 北部 南部 西部 中部 位于 一带 地区 境内\n",
      "Nearest to 土地: 大部分 拥有 面积 大量 其中 当地 部分 分别\n",
      "Nearest to 能力: 需要 并且 利用 可以 方式 或者 而且 能够\n",
      "Nearest to 帮助: 他们 并且 得到 为了 自己 没有 但是 因为\n",
      "Nearest to 交通: 中心 建设 重要 方面 为主 发展 主要 经济\n",
      "Nearest to 皇帝: 统治 罗马 国王 去世 父亲 不久 建立 帝国\n",
      "Nearest to 玩家: 选择 允许 透过 角色 利用 方式 能够 直接\n",
      "Nearest to 进一步: 为了 得到 从而 不断 并且 发展 必须 进行\n",
      "Nearest to 议会: 政府 人民 选举 任命 实行 代表 中华人民共和国 通过\n",
      "Nearest to 明显: 比较 相当 而且 具有 一定 可能 情况 非常\n",
      "Nearest to 支援: 提供 版本 技术 开发 系统 采用 透过 功能\n",
      "Nearest to 兴建: 建成 建造 计划 建设 连接 一座 完成 修建\n",
      "Nearest to 取消: 决定 宣布 随后 要求 最终 之后 于是 其后\n",
      "Average loss at 765000 is 10.655786\n",
      "Average loss at 770000 is 10.612005\n",
      "Average loss at 775000 is 10.630007\n",
      "Average loss at 780000 is 10.616247\n",
      "Average loss at 785000 is 10.667910\n",
      "Average loss at 790000 is 10.796858\n",
      "Average loss at 795000 is 10.748381\n",
      "Average loss at 800000 is 10.822990\n",
      "Average loss at 805000 is 10.822194\n",
      "Average loss at 810000 is 10.892500\n",
      "Average loss at 815000 is 10.994764\n",
      "the training time in one epoch: 222.447374\n",
      "Average loss at 820000 is 10.921762\n",
      "Average loss at 825000 is 10.078835\n",
      "Average loss at 830000 is 10.217161\n",
      "Average loss at 835000 is 10.112510\n",
      "Average loss at 840000 is 10.282731\n",
      "Average loss at 845000 is 10.557102\n",
      "Average loss at 850000 is 10.365526\n",
      "Average loss at 855000 is 10.412312\n",
      "Average loss at 860000 is 10.350241\n",
      "Average loss at 865000 is 10.501823\n",
      "Average loss at 870000 is 10.702191\n",
      "Average loss at 875000 is 10.648926\n",
      "the training time in one epoch: 220.650687\n",
      "Nearest to 实际上: 不是 但是 因为 就是 所以 没有 因此 这个\n",
      "Nearest to 出任: 担任 任命 时任 同年 邀请 主席 先后 成立\n",
      "Nearest to 网络: 信息 系统 服务 提供 相关 技术 使用 用于\n",
      "Nearest to 正在: 开始 计划 完成 已经 进行 同时 经过 目前\n",
      "Nearest to 能量: 释放 电子 产生 相同 吸收 它们 或是 可以\n",
      "Nearest to 关系: 之间 因此 因为 但是 使得 并且 所以 而且\n",
      "Nearest to 知道: 我们 应该 看到 只要 人们 那些 即使 自己\n",
      "Nearest to 组成: 包括 所有 其他 部分 分别 一个 称为 以及\n",
      "Nearest to 电视台: 广东 播放 东京 广播 播出 南方 节目 制作\n",
      "Nearest to 四个: 三个 分为 两个 分别 几个 一个 其中 部分\n",
      "Nearest to 基督教: 宗教 传统 教会 西方 早期 信仰 民间 视为\n",
      "Nearest to 少数: 大多数 大部分 其他 只有 以外 一些 除了 很多\n",
      "Nearest to 科学家: 发现 认为 学者 著名 指出 后来 许多 大多数\n",
      "Nearest to 教授: 担任 该校 大学 学校 学生 学习 学院 研究\n",
      "Nearest to 最初: 另外 作为 后来 同时 除了 称为 虽然 此外\n",
      "Nearest to 发现: 这些 但是 因此 称为 虽然 不过 因为 然而\n",
      "Nearest to 两个: 一个 三个 称为 所有 就是 这个 分别 另外\n",
      "Nearest to 有效: 能够 无法 不能 得到 为了 需要 利用 避免\n",
      "Nearest to 实际: 情况 使得 所以 因此 并且 而且 需要 但是\n",
      "Nearest to 东部: 北部 南部 西部 中部 位于 一带 地区 境内\n",
      "Nearest to 土地: 面积 大部分 拥有 大量 当地 其中 部分 全部\n",
      "Nearest to 能力: 需要 能够 利用 并且 方式 可以 而且 使得\n",
      "Nearest to 帮助: 他们 并且 得到 为了 自己 没有 但是 然而\n",
      "Nearest to 交通: 中心 建设 重要 方面 发展 经济 为主 主要\n",
      "Nearest to 皇帝: 统治 国王 罗马 去世 父亲 不久 帝国 自称\n",
      "Nearest to 玩家: 选择 允许 透过 角色 利用 能够 方式 需要\n",
      "Nearest to 进一步: 为了 得到 从而 不断 并且 发展 无法 因而\n",
      "Nearest to 议会: 政府 任命 人民 选举 代表 通过 实行 中华人民共和国\n",
      "Nearest to 明显: 相当 比较 一定 而且 具有 非常 可能 情况\n",
      "Nearest to 支援: 提供 版本 技术 开发 采用 系统 功能 透过\n",
      "Nearest to 兴建: 建成 建造 计划 建设 修建 连接 一座 完成\n",
      "Nearest to 取消: 决定 宣布 要求 随后 最终 于是 之后 其后\n",
      "total training time: 3387.6741383075714\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "with open('./words_data_saving.p', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "\n",
    "\n",
    "class GloVeModel:\n",
    "    def __init__(self, embedding_size=300, context_size=2, max_vocab_size=100000, min_occurrences=1,\n",
    "                 scaling_factor=3 / 4, cooccurrence_cap=100, batch_size=512, learning_rate=0.1, valid_size=32):\n",
    "        self.embedding_size = embedding_size\n",
    "        if isinstance(context_size, tuple):\n",
    "            self.left_context, self.right_context = context_size\n",
    "        elif isinstance(context_size, int):\n",
    "            self.left_context = self.right_context = context_size\n",
    "        else:\n",
    "            raise ValueError(\"`context_size` should be an int or a tuple of two ints\")\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cooccurrence_cap = cooccurrence_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.valid_size = valid_size\n",
    "        self.__words = None\n",
    "        self.__word_to_id = None\n",
    "        self.__cooccurrence_matrix = None\n",
    "        self.__embeddings = None\n",
    "\n",
    "    def corpus_to_graph(self, corpus):\n",
    "        self.fit_corpus(corpus, self.max_vocab_size, self.min_occurrences,\n",
    "                        self.left_context, self.right_context)\n",
    "        self.build_graph()\n",
    "\n",
    "    def fit_corpus(self, corpus, vocab_size, min_occurrences, left_size, right_size):\n",
    "\n",
    "        # 如果 key 存在，就返回 key 对应的 value，如果 key 不存在，就返回默认值0。\n",
    "        cooccurrence_counts = defaultdict(float)\n",
    "        # corpus的结构：list[words 1, ..., words n]\n",
    "        word_counts = Counter(corpus)\n",
    "        # word_counts.update(corpus)\n",
    "        for l_context, word, r_context in _context_windows(corpus, left_size, right_size):\n",
    "            # 计数一个window内的共现频数；左右两个区域\n",
    "            for i, context_word in enumerate(l_context[::-1]):\n",
    "                # 中心词的左右词频数为1，其它离越远频数越小；如不存在则创建新的键(word, context_word)，并赋值\n",
    "                cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "            for i, context_word in enumerate(r_context):\n",
    "                cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "\n",
    "        if len(cooccurrence_counts) == 0:\n",
    "            raise ValueError(\"No coccurrences in corpus. Did you try to reuse a generator?\")\n",
    "\n",
    "        # 选取vocab_size个常见词汇\n",
    "        self.__words = [word for word, count in word_counts.most_common(vocab_size)\n",
    "                        if count >= min_occurrences]\n",
    "        self.__word_to_id = {word: i for i, word in enumerate(self.__words)}\n",
    "\n",
    "        # 构造共现矩阵；将{(word1, word2): counts} 转换为 {(ID1, ID2): counts}\n",
    "        self.__cooccurrence_matrix = {\n",
    "            (self.__word_to_id[words[0]], self.__word_to_id[words[1]]): count\n",
    "            for words, count in cooccurrence_counts.items()\n",
    "            if words[0] in self.__word_to_id and words[1] in self.__word_to_id}\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default(), self.graph.device(device_for_node):\n",
    "            # 通过最大共现数抑制常见词；缩放因子小于1大于0会加强低频词的重要性\n",
    "            count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n",
    "                                    name='max_cooccurrence_count')\n",
    "            scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n",
    "                                         name=\"scaling_factor\")\n",
    "\n",
    "            # 生成验证样本\n",
    "            self.valid_examples = np.random.choice(a=1000, size=self.valid_size, replace=False)\n",
    "\n",
    "            self.center_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                               name=\"center_words\")\n",
    "            self.context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name=\"context_words\")\n",
    "            self.cooccurrences = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                                name=\"co-occurrence_count\")\n",
    "\n",
    "            focal_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"center_embeddings\")\n",
    "            context_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"context_embeddings\")\n",
    "\n",
    "            center_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='center_biases')\n",
    "            context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                         name=\"context_biases\")\n",
    "\n",
    "            focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.center_input)\n",
    "            context_embedding = tf.nn.embedding_lookup([context_embeddings], self.context_input)\n",
    "            focal_bias = tf.nn.embedding_lookup([center_biases], self.center_input)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self.context_input)\n",
    "\n",
    "            # 定义原论文中的加权函数f(x)\n",
    "            weighting_factor = tf.minimum(\n",
    "                1.0,\n",
    "                tf.pow(\n",
    "                    tf.div(self.cooccurrences, count_max),\n",
    "                    scaling_factor))\n",
    "\n",
    "            # 因为单个中心词向量只与单个上下文词向量做向量乘法，所以不能使用tf.matmul；这里先对应元素相乘再相加可得到类似的结果\n",
    "            embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "            log_cooccurrences = tf.log(tf.to_float(self.cooccurrences))\n",
    "\n",
    "            # 列表元素对应相加\n",
    "            distance_expr = tf.square(tf.add_n([\n",
    "                embedding_product,\n",
    "                focal_bias,\n",
    "                context_bias,\n",
    "                tf.negative(log_cooccurrences)]))\n",
    "\n",
    "            single_losses = tf.multiply(weighting_factor, distance_expr)\n",
    "            self.total_loss = tf.reduce_sum(single_losses)\n",
    "            self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n",
    "                self.total_loss)\n",
    "\n",
    "            self.combined_embeddings = tf.add(focal_embeddings, context_embeddings,\n",
    "                                              name=\"combined_embeddings\")\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.combined_embeddings), 1, keep_dims=True))\n",
    "            self.combined_embeddings = self.combined_embeddings/norm\n",
    "            valid_embeddings = tf.nn.embedding_lookup(self.combined_embeddings, self.valid_examples)\n",
    "            self.similarity = tf.matmul(valid_embeddings, self.combined_embeddings, transpose_b=True)\n",
    "\n",
    "    def train(self, num_epochs, summary_interval=5000):\n",
    "\n",
    "        batches = self.prepare_batches()\n",
    "        total_steps = 0\n",
    "        average_loss = 0\n",
    "\n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            start = time.time()\n",
    "            for epoch in range(num_epochs):\n",
    "                shuffle(batches)\n",
    "\n",
    "                # 遍历完列表即一个Epoch\n",
    "                for batch_index, batch in enumerate(batches):\n",
    "                    i_s, j_s, counts = batch\n",
    "                    # 不满批量数则跳过\n",
    "                    if len(counts) != self.batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        self.center_input: i_s,\n",
    "                        self.context_input: j_s,\n",
    "                        self.cooccurrences: counts}\n",
    "                    _, iter_loss = session.run([self.optimizer, self.total_loss], feed_dict=feed_dict)\n",
    "\n",
    "                    average_loss += iter_loss\n",
    "                    total_steps += 1\n",
    "                    if total_steps % summary_interval == 0:\n",
    "                        average_loss /= summary_interval\n",
    "                        print('Average loss at %d is %f' % (total_steps, average_loss))\n",
    "                        average_loss = 0\n",
    "                print('the training time in one epoch: %f' % (time.time() - start))\n",
    "                start = time.time()\n",
    "                \n",
    "                if epoch % 2 ==0:\n",
    "                    sim = self.similarity.eval()\n",
    "                    for i in range(self.valid_size):\n",
    "                        valid_word = self.__words[self.valid_examples[i]]\n",
    "                        top_k = 8\n",
    "                        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                        log_str = 'Nearest to %s:' % valid_word\n",
    "                        for k in range(top_k):\n",
    "                            close_word = self.__words[nearest[k]]\n",
    "                            log_str = '%s %s' % (log_str, close_word)\n",
    "                        print(log_str)\n",
    "\n",
    "            self.__embeddings = self.combined_embeddings.eval()\n",
    "\n",
    "    def prepare_batches(self):\n",
    "        if self.__cooccurrence_matrix is None:\n",
    "            raise NotFitToCorpusError(\n",
    "                \"Need to fit model to corpus before preparing training batches.\")\n",
    "        cooccurrences = [(word_ids[0], word_ids[1], count)\n",
    "                         for word_ids, count in self.__cooccurrence_matrix.items()]\n",
    "\n",
    "        # zip()会将两个列表相同位置的元素组成一个元组；zip(*)可理解为解压，返回二个列表\n",
    "        # i_indices, j_indices 及计数是否对称 ？？\n",
    "        i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "        # 返回的列表，每一个元素是i,j,counts组成的批量数据\n",
    "        return list(batchify(self.batch_size, i_indices, j_indices, counts))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.__words)\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if self.__words is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n",
    "        return self.__words\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if self.__embeddings is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings\n",
    "\n",
    "    def id_for_word(self, word):\n",
    "        if self.__word_to_id is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n",
    "        return self.__word_to_id[word]\n",
    "\n",
    "\n",
    "def _context_windows(region, left_size, right_size):\n",
    "    \"\"\"针对语料库的每一个词构建一个window\"\"\"\n",
    "    for i, word in enumerate(region):\n",
    "        start_index = i - left_size\n",
    "        end_index = i + right_size\n",
    "        left_context = window(region, start_index, i - 1)\n",
    "        right_context = window(region, i + 1, end_index)\n",
    "\n",
    "        # 返回一个iterable对象，对象的每一个元素是(left_context, word, right_context)\n",
    "        yield (left_context, word, right_context)\n",
    "\n",
    "\n",
    "def window(region, start_index, end_index):\n",
    "    \"\"\"\n",
    "    从 `start_index`到 `end_index`为一个词构建一个列表；\n",
    "    如果头尾不满足window的长度，补充`NULL_WORD`.\n",
    "    \"\"\"\n",
    "    last_index = len(region) + 1\n",
    "    selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n",
    "    return selected_tokens\n",
    "\n",
    "\n",
    "def device_for_node(n):\n",
    "    # 矩阵乘法OP使用GPU\n",
    "    if n.type == \"MatMul\":\n",
    "        return \"/gpu:0\"\n",
    "    else:\n",
    "        return \"/cpu:0\"\n",
    "\n",
    "\n",
    "# 可变参数，接收一个元组\n",
    "def batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        # 每一个批量，将批量个i,j,counts打包为元组；所有元组组成一个Epoch\n",
    "        yield tuple(sequence[i:i + batch_size] for sequence in sequences)\n",
    "\n",
    "\n",
    "class NotTrainedError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NotFitToCorpusError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    start = time.time()\n",
    "    model = GloVeModel(batch_size=512)\n",
    "    print('Modelling time:', time.time()-start)\n",
    "    start = time.time()\n",
    "    model.corpus_to_graph(corpus=words)\n",
    "    print('fit corpus time:', time.time() - start)\n",
    "    start = time.time()\n",
    "    model.train(num_epochs=15)\n",
    "    print('total training time:', time.time() - start)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GloVe_CN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
